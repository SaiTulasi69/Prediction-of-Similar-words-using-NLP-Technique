{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPmTpsQlBMuUtujmLhXbnZ3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SaiTulasi69/Prediction-of-Similar-words-using-NLP-Technique/blob/main/Prediction_of_Similar_words_using_NLP_Technique.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import bs4 as bs\n",
        "import urllib.request\n",
        "import re\n",
        "import nltk"
      ],
      "metadata": {
        "id": "3zyioPn0bJ1d"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scrapped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/Machine_learning')\n",
        "article = scrapped_data .read()\n",
        "\n",
        "parsed_article = bs.BeautifulSoup(article,'lxml')\n",
        "\n",
        "paragraphs = parsed_article.find_all('p')\n",
        "\n",
        "article_text = \"\"\n",
        "\n",
        "for p in paragraphs:\n",
        "    article_text += p.text"
      ],
      "metadata": {
        "id": "J7EblHxibJCG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    import string\n",
        "    from nltk.corpus import stopwords\n",
        "    import nltk\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "\n",
        "\n",
        "class PreProcessText(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def __remove_punctuation(self, text):\n",
        "        \"\"\"\n",
        "        Takes a String\n",
        "        return : Return a String\n",
        "        \"\"\"\n",
        "        message = []\n",
        "        for x in text:\n",
        "            if x in string.punctuation:\n",
        "                pass\n",
        "            else:\n",
        "                message.append(x)\n",
        "        message = ''.join(message)\n",
        "\n",
        "        return message\n",
        "\n",
        "    def __remove_stopwords(self, text):\n",
        "        \"\"\"\n",
        "        Takes a String\n",
        "        return List\n",
        "        \"\"\"\n",
        "        words= []\n",
        "        for x in text.split():\n",
        "\n",
        "            if x.lower() in stopwords.words('english'):\n",
        "                pass\n",
        "            else:\n",
        "                words.append(x)\n",
        "        return words\n",
        "\n",
        "\n",
        "    def token_words(self,text=''):\n",
        "        \"\"\"\n",
        "        Takes String\n",
        "        Return Token also called  list of words that is used to\n",
        "        Train the Model\n",
        "        \"\"\"\n",
        "        message = self.__remove_punctuation(text)\n",
        "        words = self.__remove_stopwords(message)\n",
        "        return words"
      ],
      "metadata": {
        "id": "PgtgQM9kbGn7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "flag = nltk.download(\"stopwords\")\n",
        "\n",
        "if (flag == \"False\" or flag == False):\n",
        "    print(\"Failed to Download Stop Words\")\n",
        "else:\n",
        "    print(\"Downloaded Stop words ...... \")\n",
        "    helper = PreProcessText()\n",
        "    #words = helper.token_words(text=txt)\n",
        "    words = helper.token_words(text=article_text)"
      ],
      "metadata": {
        "id": "betMGicabBac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "147cda24-9eee-49a4-ce67-24d205f4ca7f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded Stop words ...... \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Dqz3xIFda9Mc"
      },
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec([words], size=100, window=5, min_count=1, workers=4)"
      ],
      "metadata": {
        "id": "dvjAkS-oa_41",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd60bd96-f561-48e8-aae4-23e04b3973ea"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.base_any2vec:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = model.wv.vocab"
      ],
      "metadata": {
        "id": "Z3vItDwCa_1f"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sim_words = model.wv.most_similar('machine')\n"
      ],
      "metadata": {
        "id": "YNgPniUla_zt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sim_words"
      ],
      "metadata": {
        "id": "-jhWa9bha_xU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee1c7da9-791a-43d8-eb72-5f8b916d8bf0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('medical', 0.356807678937912),\n",
              " ('problems', 0.32222580909729004),\n",
              " ('falls', 0.31818410754203796),\n",
              " ('problem', 0.31490734219551086),\n",
              " ('Grand', 0.31125521659851074),\n",
              " ('receiver', 0.3035690188407898),\n",
              " ('conclude', 0.30062562227249146),\n",
              " ('change', 0.2854427695274353),\n",
              " ('text', 0.27817869186401367),\n",
              " ('Machine', 0.276690274477005)]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = model[model.wv.vocab]\n"
      ],
      "metadata": {
        "id": "jrnDzptZa_u_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44e6597b-0755-4455-d208-ba2e3014b19e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-c60322eee7e8>:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
            "  X = model[model.wv.vocab]\n"
          ]
        }
      ]
    }
  ]
}